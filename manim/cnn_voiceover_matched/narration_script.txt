Let's dive into the math behind Convolutional Neural Networks. We'll break down each component, step by step.

First, how do we represent an image mathematically? Think of a grayscale image.

Each pixel in that image has a specific intensity value, ranging from 0 to 255. This lets us represent the entire image as a matrix.

We can denote this image matrix as *I*, where *I* belongs to the set of real numbers raised to the power of *H* times *W*. *H* represents the height, and *W* the width, of the image in pixels. So, a 5x5 grayscale image would be represented as a 5x5 matrix.

Now, let's move on to the heart of CNNs: the convolution operation.

Convolution might seem intimidating at first, but it's just a structured way of combining two matrices. We have our input image, and a smaller matrix called a kernel or filter. The convolution operation calculates how much the filter "matches" different regions of the image.

Here's the formula: (I * K)<sub>i,j</sub> equals the sum over *m* from 0 to *k<sub>h</sub>*-1, and *n* from 0 to *k<sub>w</sub>*-1 of I<sub>i+m, j+n</sub> times K<sub>m,n</sub>.

Let's break that down with an example. Imagine a 3x3 kernel sliding across a 5x5 image. At each location, we perform an element-wise multiplication between the kernel and the corresponding section of the image, and then sum up all the results.

Let's look at the calculation step by step. We take our input region and our kernel, multiply corresponding elements, and then sum those products. So we get (1)(1) + (2)(0) + (3)(-1), and so on. Adding all those terms together gets us a value of -6.

This single value, -6, becomes one element in our output feature map. Think of it as the filter's response to that specific part of the input image.

Next up: activation functions.

After the convolution, we typically apply a non-linear activation function. One of the most popular is ReLU, or Rectified Linear Unit.

The ReLU function is quite simple: f(x) = max(0, x). In other words, if the input *x* is positive, the output is *x*. If *x* is negative or zero, the output is zero. We can express this as a piecewise function. *x* if *x* is greater than zero, and zero if *x* is less than or equal to zero.

Graphically, ReLU looks like a ramp. It's zero for negative values and then a straight line with a slope of one for positive values. This non-linearity is crucial, allowing neural networks to learn complex patterns.

And for backpropagation, we need the derivative of the ReLU function. This is 1 if x is greater than zero, and zero if x is less than or equal to zero.

Now, let's discuss pooling operations.

Pooling is used to reduce the spatial size of the feature maps, effectively summarizing information from the previous layer. A common type is max pooling.

With max pooling, we define a pooling window – let's say 2x2 – and slide it across the input feature map. In each window, we simply take the maximum value.

Mathematically: P<sub>i,j</sub> equals the max of I<sub>i+m, j+n</sub>, where m and n are within the pooling region.

Consider a 4x4 input. We can perform max pooling with a 2x2 window and a stride of 2, resulting in a 2x2 output. The maximum value in each 2x2 region becomes the corresponding element in the output.

This reduces the spatial dimensions, helps to retain the most important features, and makes the network more robust to variations in the input.

Let’s talk about feature maps.

A single convolutional layer often uses multiple filters, and each filter detects different features in the image, such as edges, textures, or shapes.

For instance, one filter might be designed to detect vertical edges, another to find horizontal edges, and yet another to identify diagonal lines.

Applying these filters to the same input image generates multiple output feature maps. So, applying three different kernels would result in three feature maps, or channels. Each one highlights a different aspect of the input.

Finally, let's touch on backpropagation.

Backpropagation is how the network learns. It relies on calculating gradients of a loss function with respect to the network's parameters, such as the kernel weights. A typical loss function is cross-entropy, mathematically represented here.

The goal is to minimize this loss. To do this, we calculate the gradient of the loss with respect to each kernel weight. The formula shown here tells us how much each weight contributed to the overall error.

Then, we update the weights using gradient descent. We nudge each weight a little bit in the direction that reduces the loss. This update is controlled by the learning rate, often a small value such as 0.001.

So, we subtract from K<sub>m,n</sub><sup>old</sup> the learning rate, eta, times the gradient.

So, to summarize, a CNN architecture typically involves the following steps:

1.  Input an image represented as a matrix with height, width, and color channels.
2.  Convolve the input with filters to extract local features.
3.  Apply an activation function like ReLU to introduce non-linearity.
4.  Use pooling to reduce the dimensions and retain important features.
5.  Repeat these convolutional, activation, and pooling layers multiple times.
6.  Flatten the output into a vector.
7.  Pass it through dense layers for classification.
8.  And finally, use backpropagation to learn the optimal filter weights.

In essence, we can think of a CNN as a function that takes an image as input and produces a classification as output: Y equals some function *f* of Dense of Flatten of Pooling of ReLU of I convolved with K.