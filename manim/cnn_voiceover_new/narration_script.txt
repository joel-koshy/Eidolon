Convolutional Neural Networks are powerful tools, but what's actually going on under the hood? Let's dive into a complete mathematical explanation.

First, let's talk about how computers see images. Specifically, how they represent them.

Think of a grayscale image. We can represent it as a matrix, I, with dimensions H by W. H is the height, W is the width. Each element in this matrix corresponds to a pixel's intensity value. These values range from 0 to 255, representing the brightness of that pixel, from black to white.

Okay, so we have our image represented mathematically. Now, let's get to the heart of CNNs: the convolution operation.

The convolution operation, denoted by the asterisk, defines how we apply a kernel or filter, K, across our image, I.  This formula, written as (I * K) at position i,j, is essentially a weighted sum. We sum over the kernel height k_h and kernel width k_w, multiplying the corresponding elements of the input region and the kernel, and adding them all up.

Let's break that down with a concrete example, using a 3x3 kernel on a 5x5 image. Here's an input region – a 3x3 section of our original image – and our kernel, often called a filter.

The convolution operation involves element-wise multiplication, indicated by this symbol, followed by summation. So, we multiply each corresponding number in the input region and the kernel: 1 times 1, 2 times 0, 3 times -1, and so on. Adding these products together gives us this calculation. We're summing the products across each row... and that simplifies down to... -6.

This value, -6, becomes the output value at the corresponding location in our feature map. We can visualize it like this. We have a single number.

Alright, so we have a resulting number after a convolution, but what about activation functions?

Activation functions introduce non-linearity into our network, allowing it to learn complex patterns. A very common one is ReLU, the Rectified Linear Unit. ReLU is elegantly simple: f(x) equals the maximum of 0 and x. This means if x is positive, we output x. If x is zero or negative, we output zero.  Its graph looks like this. Notice how it introduces a "kink" at zero.

This seemingly simple function has a derivative that's also straightforward.  If x is greater than zero, the derivative, f prime of x, is 1. If x is less than or equal to zero, the derivative is zero.  We'll need this derivative during backpropagation to train the network.

Now, let's discuss pooling operations.

Pooling operations reduce the spatial dimensions of our feature maps. Max pooling is a popular choice. For example, we can choose a 2x2 pooling window. Max pooling takes the maximum value within each of these windows.

Written mathematically, P at position i,j equals the maximum value within the "pool," denoted by a region from I at i+m, j+n.

Let's illustrate with a 4x4 input that we want to pool down to 2x2. We're taking the input and creating an output that's smaller. Think of this arrow visualizing the process.

We divide our 4x4 input into 2x2 regions.  In the first region, the max value is 6. In the second, 4.  In the third, 8.  And in the last region, 9. These max values form our 2x2 output.

This process reduces the spatial dimensions, but it also helps retain the most important features by selecting the maximum activations.

Now, let's see what happens if we use multiple filters.

Each filter is designed to detect different features in the image. For example, we might have one filter that's good at detecting vertical edges.  Another for horizontal edges.  And yet another to detect diagonal lines.

Here's the matrix representation of three different kernels: one emphasizes vertical edges, one horizontal, and another, diagonals. Applying each of these kernels to the same input image will generate a different feature map, representing how strongly each of these features are present.

These different feature maps are often stacked together creating multiple output channels. So one filter turns into one channel.

So, how does the network learn the values within these kernels? It's all thanks to backpropagation.

Backpropagation is the process of calculating gradients and updating the network's weights to minimize a loss function. A common loss function is cross-entropy loss, denoted here. The loss function calculates the difference between the ground truth labels, y_i, and the network's predictions, y_hat_i.

To update the kernel weights, we need to calculate the gradient of the loss function with respect to those weights.

This formula shows how to calculate the gradient of the loss function, L, with respect to a specific kernel weight, K at m,n. It involves summing over the output feature map indices i and j, and multiplying the gradient of the loss function with respect to the output, O, by the corresponding input region value, I.

Finally, we update the kernel weights using gradient descent.

Here, the new kernel weight, K at m,n, is equal to the old kernel weight minus the learning rate, eta, times the gradient we just calculated. The learning rate, often a small value like 0.001, controls the step size during optimization.

So let's zoom out and summarize the overall architecture of a CNN.

Here's the breakdown: First, we start with an input image represented as a matrix with dimensions H x W x C, where C represents the number of channels (e.g., 3 for RGB). Next, we perform convolution to extract local features. After each convolution, we often apply an activation function like ReLU to introduce non-linearity. Pooling operations then reduce the dimensions of the feature maps. We often stack these layers, repeating the convolution, activation, and pooling steps multiple times. The output is then flattened into a vector, passed through dense layers for classification, and then finally the backpropagation learns optimal filters.

We can express this flow concisely: Y equals f, which is the final classification result, of a dense layer acting on a flattened layer, which in turn acts on a pooling layer, which in turn acts on a rectified linear unit (ReLU) layer, which in turn acts on the initial convolution of the input image I with the kernel K.

Hopefully, that gives you a better understanding of the math behind convolutional neural networks.