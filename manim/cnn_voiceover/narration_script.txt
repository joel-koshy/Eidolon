Convolutional Neural Networks, or CNNs, have revolutionized fields like image recognition. But what's the math *really* doing? Let's dive deep and uncover the complete mathematical explanation behind CNNs.

First, how do we represent an image to a computer? Let's start with a grayscale image.

The input image can be represented as a matrix, `I`, with dimensions `H` by `W`, where `H` is the height and `W` is the width of the image. Each element in this matrix represents the intensity value of a pixel, ranging from 0 to 255. So, each pixel contains an intensity value from 0 to 255, representing how bright that pixel is.

Now, let's talk about the convolution operation, the heart of CNNs.

The convolution operation, denoted as `I * K`, calculates the output at position `i, j` by summing the element-wise product of a kernel `K` and a corresponding region of the input image `I`. Mathematically, this is represented as... well, the formula you see here.

Let's break down a simple example. Consider a 3x3 kernel sliding across a 5x5 image.

We'll take a 3x3 region from the input image and a 3x3 kernel, also known as a filter. The kernel usually contains pre-defined values to detect certain features, such as edges or corners.
Now, we perform an element-wise multiplication between the input region and the kernel, and then sum all the results.

So, multiplying these two matrices element-wise, we get this. Adding all of those terms gives us -6.

This value, -6 in this case, is the output of the convolution operation for that specific region, representing the feature detected by the kernel. Let's place that result into our output.

Next up: activation functions.

After the convolution, we typically apply an activation function. A very common one is ReLU, which stands for Rectified Linear Unit.

ReLU is defined as `f(x) = max(0, x)`. In simpler terms, if the input `x` is positive, the output is `x`. If the input is zero or negative, the output is zero. ReLU introduces non-linearity into the network, allowing it to learn more complex patterns.
Here's what ReLU looks like graphically.

The derivative of ReLU is also important for backpropagation. If `x` is positive, the derivative is 1. If `x` is zero or negative, the derivative is 0. This simple derivative helps in efficiently training the network.

Now, let's move on to pooling operations.

Pooling operations reduce the spatial dimensions of the feature maps, decreasing the number of parameters and computational complexity. A common type is max pooling.

Max pooling, with a size of 2x2 for example, takes the largest value within each 2x2 region. Mathematically, `P_{i,j}` is the maximum value in the pool within the given input region.

Letâ€™s say we have a 4x4 input.

We divide this into 2x2 regions. In the first region, the max value is 6. The result of the first max pool region is 6. The same logic goes for the rest of the max pool, where we select the maximum value in each region.

Max pooling reduces the spatial dimensions, while retaining the most important features from each region. It reduces the size of the data, but maintains the important features that have been extracted.

So, how do multiple filters come into play?

Each filter is designed to detect different features within the input image. For example, one filter might detect vertical edges, another horizontal edges, and yet another diagonal lines.

We can visualize some kernels that each detect a particular feature.

Applying these filters to the same input image generates multiple feature maps, each representing a different characteristic of the input. These feature maps can be thought of as channels, similar to the color channels in a color image.

Finally, let's discuss backpropagation.

During training, we need to adjust the kernel weights to minimize the difference between the network's predictions and the actual labels. This is achieved through backpropagation.

We start with a Loss Function, such as Cross-Entropy. The formula you see here measures the difference between the predicted output `y_hat` and the true label `y`. The goal is to minimize this loss.

Then we can compute the gradient of the loss function with respect to the kernel weights, indicating how much each weight contributes to the overall error. This can be expressed as... the formula you see here.

The kernel weights are then updated using gradient descent, adjusting them in the opposite direction of the gradient, scaled by a learning rate `eta`. The new weight `K_new` equals the old weight `K_old` minus the learning rate `eta` times the gradient of the loss with respect to the weight.

The learning rate, often a small value like 0.001, controls the step size during the weight update process.

So to summarize, a CNN takes an image as input, applies convolutional filters to extract features, uses activation functions like ReLU to introduce non-linearity, employs pooling to reduce dimensions, and then uses backpropagation to learn the optimal filter weights. The most complex CNN architecture is simply repeating these steps in succession.

In essence, the entire process can be represented by this formula: `Y = f(Dense(Flatten(Pool(ReLU(I * K)))))`.